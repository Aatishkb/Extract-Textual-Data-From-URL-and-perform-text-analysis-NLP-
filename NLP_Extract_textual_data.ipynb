{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66016ddd-9dd3-4155-bf82-5bd52a379043",
   "metadata": {},
   "source": [
    "# Blackcoffer\n",
    " - Test Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbdc06d-6ad3-4527-a9bc-fc1aeb93de8f",
   "metadata": {},
   "source": [
    "# Extract textual data from URL and perform Sentimental analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e0ab7d-b26a-4626-9648-c7272b179a42",
   "metadata": {},
   "source": [
    "## Approaches :-\n",
    "\n",
    "1. **Data Extraction:**\n",
    "   - Utilize Python programming to extract data from the provided URLs.\n",
    "   - Use libraries such as BeautifulSoup, Selenium, or Scrapy for web crawling.\n",
    "   - Extract only the article title and text, excluding headers, footers, and other unnecessary content.\n",
    "   - Save the extracted articles in text files with URL_ID as their filenames.\n",
    "\n",
    "2. **Data Analysis:**\n",
    "   - Employ Python programming for text analysis of the extracted articles.\n",
    "   - Compute the variables specified in the \"Text Analysis.docx\" file, such as positive score, negative score, polarity score, etc.\n",
    "   - Refer to the definitions provided in the analysis document for each variable.\n",
    "   - Perform computations for each article and save the output in the same order as specified in the \"Output Data Structure.xlsx\" file.\n",
    "\n",
    "3. **Output Data Structure:**\n",
    "   - Ensure the output variables match the format specified in the \"Output Data Structure.xlsx\" file.\n",
    "   - Include all input variables from the \"Input.xlsx\" file along with the computed variables.\n",
    "   - Save the output in CSV or Excel format.\n",
    "\n",
    "4. **Output Variables:**\n",
    "  - We have to calculate:\n",
    "  - 1.\tPOSITIVE SCORE\n",
    "  - 2.\tNEGATIVE SCORE\n",
    "  - 3.\tPOLARITY SCORE\n",
    "  - 4.\tSUBJECTIVITY SCORE\n",
    "  - 5.\tAVG SENTENCE LENGTH\n",
    "  - 6.\tPERCENTAGE OF COMPLEX WORDS\n",
    "  - 7.\tFOG INDEX\n",
    "  - 8.\tAVG NUMBER OF WORDS PER SENTENCE\n",
    "  - 9.\tCOMPLEX WORD COUNT\n",
    "  - 10.\tWORD COUNT\n",
    "  - 11.\tSYLLABLE PER WORD\n",
    "  - 12.\tPERSONAL PRONOUNS\n",
    "  - 13.\tAVG WORD LENGTH\n",
    "                  \n",
    "5. **Finaly:** \n",
    "   - After Calculating all the above Parameters Result can be store in .py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a6052e-bed0-41f5-bbe5-21f281253d8a",
   "metadata": {},
   "source": [
    "## \tExplaining approach solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdffc86d-485e-41a9-b4e2-19431e3224ce",
   "metadata": {},
   "source": [
    " - Use Python for both data extraction and analysis.\n",
    " - Utilize libraries such as BeautifulSoup or Scrapy for web crawling.\n",
    " - Extract article text and save it in text files with URL_ID as filenames.\n",
    " - Perform textual analysis to compute specified variables.\n",
    " - Calculate sentiment scores, readability, and other metrics.\n",
    " - Follow defined algorithms for sentiment analysis.\n",
    " - Clean text using stop words lists and create dictionaries for positive and negative words.\n",
    " - Extract derived variables such as Positive Score, Negative Score, Polarity Score, and Subjectivity Score.\n",
    " - Calculate readability metrics like Average Sentence Length, Percentage of Complex Words, and Fog Index.\n",
    " - Count complex words, total words, syllables per word, personal pronouns, and average word length.\n",
    " - Ensure output matches the specified format in the \"Output Data Structure.xlsx\" file.\n",
    " - Submit solution through provided Google form including .py file, output in CSV or Excel format, and instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eab5da-a047-405b-889f-74eba34139ad",
   "metadata": {},
   "source": [
    "### (1). Importing Required Libraries :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00318891-42f6-414a-89a0-77a2bd68dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915d939-c9ea-4812-9a28-fd953b4d0f18",
   "metadata": {},
   "source": [
    "## Explanation of each library:\n",
    "\n",
    "- **pandas (pd)**:\n",
    "  - pandas is a powerful data manipulation and analysis library for Python.\n",
    "  - It provides data structures like DataFrame and Series, which are widely used for data manipulation and analysis tasks.\n",
    "  - pandas allows you to easily read, write, filter, group, and visualize data.\n",
    "\n",
    "- **requests**:\n",
    "  - requests is a popular HTTP library for making HTTP requests in Python.\n",
    "  - It allows you to send HTTP/1.1 requests extremely easily.\n",
    "\n",
    "- **BeautifulSoup**:\n",
    "  - BeautifulSoup is a Python library for pulling data out of HTML and XML files.\n",
    "  - It provides simple methods and Pythonic idioms for navigating, searching, and modifying the parse tree.\n",
    "  - BeautifulSoup is commonly used for web scraping tasks.\n",
    "\n",
    "- **tqdm**:\n",
    "  - tqdm is a fast, extensible progress bar library for Python and CLI.\n",
    "  - It provides a simple and easy-to-use progress bar for loops and other iterative processes.\n",
    "  - tqdm is particularly useful when working with long-running processes to monitor progress.\n",
    "\n",
    "- **nltk**:\n",
    "  - Natural Language Toolkit (NLTK) is a leading platform for building Python programs to work with human language data.\n",
    "  - It provides easy-to-use interfaces to over 50 corpora and lexical resources, such as WordNet.\n",
    "  - NLTK also includes a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "\n",
    "- **nltk.corpus.stopwords**:\n",
    "  - nltk.corpus.stopwords provides a list of common stop words in various languages.\n",
    "  - Stop words are words that are filtered out before or after text processing during natural language processing (NLP).\n",
    "\n",
    "- **nltk.corpus.opinion_lexicon**:\n",
    "  - nltk.corpus.opinion_lexicon provides a list of positive and negative opinion words.\n",
    "  - It's commonly used in sentiment analysis tasks to determine the sentiment polarity of text.\n",
    "\n",
    "- **collections.defaultdict**:\n",
    "  - defaultdict is a subclass of the built-in dict class in Python.\n",
    "  - It returns a new dictionary-like object, which is a subclass of the dict class.\n",
    "  - defaultdict is useful when you want to have a default value for keys that haven't been set yet in the dictionary.\n",
    "\n",
    "- **nltk.tokenize.word_tokenize**:\n",
    "  - `word_tokenize` is a function from the NLTK library used for tokenizing text into words.\n",
    "  - Tokenization is the process of breaking down a text into smaller units, such as words or sentences, for further analysis.\n",
    "  - `word_tokenize` specifically tokenizes text into individual words based on whitespace and punctuation.\n",
    "\n",
    "- **nltk.corpus.cmudict**:\n",
    "  - `cmudict` is a part of the NLTK library and provides access to the Carnegie Mellon University Pronouncing Dictionary.\n",
    "  - This dictionary contains mappings between words and their phonetic representations.\n",
    "  - It is commonly used in tasks such as speech processing, text-to-speech synthesis, and syllable counting.\n",
    "  - By accessing `cmudict`, you can retrieve the phonetic representation of English words, which is useful for various linguistic analyses and applications.\n",
    "\n",
    "- **re**:\n",
    "  - Python's built-in regular expression library.\n",
    "  - Provides support for working with regular expressions.\n",
    "  - Allows for tasks like pattern matching, text manipulation, and parsing.\n",
    "  - Useful for tasks such as string cleaning, validation, and extraction.\n",
    "\n",
    "- **string**:\n",
    "  - Python's built-in module for common string operations.\n",
    "  - Contains constants for ASCII letters, digits, and punctuation characters.\n",
    "  - Provides functions for string formatting, manipulation, and testing.\n",
    "  - Helpful for tasks involving string processing, formatting, and manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80741ce6-d2da-485c-a130-fe81ced52982",
   "metadata": {},
   "source": [
    "### (2). Extract textual data from Single URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c39c18-1673-4ad3-a190-2d1140ea9d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment Effective Management of Social Media Data Extraction: Strategies for Authentication, Security, and Reliability AI Bot Audio to audio Methodology for ETL Discovery Tool using LLMA, OpenAI, Langchain Methodology for database discovery tool using openai, LLMA, Langchain Chatbot using VoiceFlow Rising IT cities and its impact on the economy, environment, infrastructure, and city life by the year 2040. Rising IT Cities and Their Impact on the Economy, Environment, Infrastructure, and City Life in Future Internet Demand’s Evolution, Communication Impact, and 2035’s Alternative Pathways Rise of Cybercrime and its Effect in upcoming Future AI/ML and Predictive Modeling Solution for Contact Centre Problems How to Setup Custom Domain for Google App Engine Application? Code Review Checklist We have seen a huge development and dependence of people on technology in recent years. We have also seen the development of AI and ChatGPT in recent years. So it is a normal thing that we will become fully dependent on technology by 2040. Information technology will be a major power for all the developing nations. As a member of a developing nation, India is rapidly growing its IT base. It has also grown some IT cities which will be the major control centres for Information technology by 2040. Rising IT cities Kolkata:- Kolkata in West Bengal is an emerging major IT hub. The new Kolkata i.e. Saltlake Sector  5, New town, Rajarhat area of Kolkata is a major IT hub. The government is giving the software companies land at almost free of cost to set up the companies there. Many large companies like Google, Microsoft, IBM, Infosys and others have set up their companies here. Kolkata has a market base of billions of dollars and is doing a great job of boosting the national economy. Impact on Economy There is a huge impact of the rising IT cities on our economy. Some of the effects are- Impact on Environment The rising IT cities will create a huge impact on the environment, the maximum of which will be harmful effects. The impact of rising IT cities on the environment is- Impact on infrastructure There are many contributions of the IT cities on infrastructure.  They are- Impact on city life With the growth of IT cities, more people will get jobs and will earn more. So the purchasing power of the people will increase. People will lead a better lifestyle. They will buy things of good brand value. The tastes and preferences of people will also change. The human development index is going to increase. People will buy good quality food and good quality cars. So the food, automobile and many other industries are going to increase. So there will be a huge impact on city life by 2040. We provide intelligence, accelerate innovation and implement technology with extraordinary breadth and depth global insights into the big data,data-driven dashboards, applications development, and information management for organizations through combining unique, specialist services and high-lvel human expertise. Contact us: hello@blackcoffer.com © All Right Reserved, Blackcoffer(OPC) Pvt. Ltd\n"
     ]
    }
   ],
   "source": [
    "def extract_article_text(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all paragraphs within the article content\n",
    "    paragraphs = soup.find_all('p')\n",
    "    \n",
    "    # Concatenate text from all paragraphs\n",
    "    article_text = ' '.join([p.get_text() for p in paragraphs])\n",
    "    \n",
    "    return article_text\n",
    "\n",
    "# Example usage\n",
    "url = \"https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/\"\n",
    "article_text = extract_article_text(url)\n",
    "print(article_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5fde1c0-9612-49ca-8d2c-cc3d6d255028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07102498-a063-4ebe-81c3-4f15232eb877",
   "metadata": {},
   "source": [
    "### (3). Extract textual data from Multiple URL(Excel File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46fbf0b5-091d-4cd2-bab7-12459da9e7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Text: 100%|███████████████████████████████████████████████████████████████| 100/100 [10:45<00:00,  6.46s/it]\n"
     ]
    }
   ],
   "source": [
    "def extract_article_text(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all paragraphs within the article content\n",
    "    paragraphs = soup.find_all('p')\n",
    "    \n",
    "    # Concatenate text from all paragraphs\n",
    "    article_text = ' '.join([p.get_text() for p in paragraphs])\n",
    "    \n",
    "    return article_text\n",
    "\n",
    "def extract_text_from_urls(input_excel, output_excel):\n",
    "    # Read the input Excel file\n",
    "    df = pd.read_excel(r'A:\\MTECH(Data Science)\\NLP\\Input.xlsx')\n",
    "    \n",
    "    # Initialize a list to store the extracted data\n",
    "    data = []\n",
    "    \n",
    "    # Iterate over each row in the dataframe\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting Text\"):\n",
    "        url_id = row['URL_ID']\n",
    "        url = row['URL']\n",
    "        \n",
    "        # Extract article text from the URL\n",
    "        article_text = extract_article_text(url)\n",
    "        \n",
    "        # Append URL id and article text to the data list\n",
    "        data.append({'URL_ID': url_id, 'Text': article_text})\n",
    "    \n",
    "    # Convert the data list to a dataframe\n",
    "    result_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save the dataframe to a CSV file\n",
    "    result_df.to_excel(output_excel, index=False)\n",
    "\n",
    "# Example usage\n",
    "input_excel = \"input.xlsx\"  # Replace with your input Excel file name\n",
    "output_excel = \"Output Data Structure.xlsx\"    # Replace with your desired output CSV file name\n",
    "extract_text_from_urls(input_excel, output_excel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffea8c65-d0c0-42b1-b67c-c471b4537294",
   "metadata": {},
   "source": [
    "### (4). Load Extracted Textual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c188869-37a3-4c18-9ca4-993030b2baa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>blackassign0053</td>\n",
       "      <td>Efficient Supply Chain Assessment: Overcoming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>blackassign0029</td>\n",
       "      <td>Efficient Supply Chain Assessment: Overcoming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>Efficient Supply Chain Assessment: Overcoming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>blackassign0063</td>\n",
       "      <td>Efficient Supply Chain Assessment: Overcoming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>blackassign0008</td>\n",
       "      <td>Efficient Supply Chain Assessment: Overcoming ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                               Text\n",
       "52  blackassign0053  Efficient Supply Chain Assessment: Overcoming ...\n",
       "28  blackassign0029  Efficient Supply Chain Assessment: Overcoming ...\n",
       "0   blackassign0001  Efficient Supply Chain Assessment: Overcoming ...\n",
       "62  blackassign0063  Efficient Supply Chain Assessment: Overcoming ...\n",
       "7   blackassign0008  Efficient Supply Chain Assessment: Overcoming ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(r'Output Data Structure.xlsx')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c40d8d-1c57-4caa-bb9e-88e8746fb326",
   "metadata": {},
   "source": [
    "### (5). Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84ef00f-36ad-4331-86c5-4f9b4bb4b8c7",
   "metadata": {},
   "source": [
    "#### (i). Cleaning Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700910b1-8e10-4e40-999a-dae8451b2411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Cleaning Stopwords: 100%|████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 36.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "# Assuming df is your DataFrame containing textual data, and 'Text' is the column name\n",
    "def clean_stopwords_with_tqdm(df):\n",
    "    tqdm.pandas(desc=\"Cleaning Stopwords\")\n",
    "    df['Text'] = df['Text'].progress_apply(clean_stopwords)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# Replace df with your DataFrame containing the textual data\n",
    "df = clean_stopwords_with_tqdm(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb12d0f7-ad65-47da-b7c8-332ddcccfcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>blackassign0063</td>\n",
       "      <td>Efficient Supply Chain Assessment : Overcoming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>blackassign0038</td>\n",
       "      <td>Efficient Supply Chain Assessment : Overcoming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>blackassign0052</td>\n",
       "      <td>Efficient Supply Chain Assessment : Overcoming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>blackassign0026</td>\n",
       "      <td>Efficient Supply Chain Assessment : Overcoming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>Efficient Supply Chain Assessment : Overcoming...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                               Text\n",
       "62  blackassign0063  Efficient Supply Chain Assessment : Overcoming...\n",
       "37  blackassign0038  Efficient Supply Chain Assessment : Overcoming...\n",
       "51  blackassign0052  Efficient Supply Chain Assessment : Overcoming...\n",
       "25  blackassign0026  Efficient Supply Chain Assessment : Overcoming...\n",
       "96  blackassign0097  Efficient Supply Chain Assessment : Overcoming..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9523ef-f8fd-4fd7-bec9-3ba590040a2c",
   "metadata": {},
   "source": [
    "#### (ii). Creating dictionary of Positive and Negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61e1c55e-373d-4564-a682-b36e2c5954ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "Creating Sentiment Dictionary: 100%|█████████████████████████████████████████████████| 100/100 [39:33<00:00, 23.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK opinion_lexicon if not already downloaded\n",
    "nltk.download('opinion_lexicon')\n",
    "\n",
    "def create_sentiment_dictionary(df):\n",
    "    positive_words = defaultdict(int)\n",
    "    negative_words = defaultdict(int)\n",
    "\n",
    "    # Iterate over each row in the dataframe\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Creating Sentiment Dictionary\"):\n",
    "        # Tokenize the text\n",
    "        tokens = nltk.word_tokenize(row['Text'])  # Assuming 'Text' is the column name containing text\n",
    "        \n",
    "        # Classify each token as positive or negative\n",
    "        for token in tokens:\n",
    "            if token in opinion_lexicon.positive():\n",
    "                positive_words[token] += 1\n",
    "            elif token in opinion_lexicon.negative():\n",
    "                negative_words[token] += 1\n",
    "\n",
    "    return positive_words, negative_words\n",
    "\n",
    "# Example usage\n",
    "positive_words, negative_words = create_sentiment_dictionary(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13e4b905-db33-4ee5-859d-8d8a969072a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'free': 10,\n",
       "             'like': 204,\n",
       "             'great': 34,\n",
       "             'lead': 38,\n",
       "             'better': 113,\n",
       "             'good': 52,\n",
       "             'intelligence': 211,\n",
       "             'innovation': 122,\n",
       "             'extraordinary': 100,\n",
       "             'smart': 31,\n",
       "             'improvements': 8,\n",
       "             'advanced': 18,\n",
       "             'revolutionary': 3,\n",
       "             'greatest': 9,\n",
       "             'celebrate': 1,\n",
       "             'effective': 36,\n",
       "             'improve': 44,\n",
       "             'improves': 6,\n",
       "             'cleaner': 4,\n",
       "             'productive': 5,\n",
       "             'significant': 70,\n",
       "             'exceeded': 2,\n",
       "             'stunning': 1,\n",
       "             'advantages': 5,\n",
       "             'well': 89,\n",
       "             'favorable': 1,\n",
       "             'fresh': 16,\n",
       "             'improved': 24,\n",
       "             'sustainable': 10,\n",
       "             'guarantee': 7,\n",
       "             'bright': 3,\n",
       "             'prosperous': 1,\n",
       "             'favor': 3,\n",
       "             'fastest-growing': 3,\n",
       "             'top': 30,\n",
       "             'skill': 10,\n",
       "             'high-quality': 4,\n",
       "             'fair': 7,\n",
       "             'proven': 6,\n",
       "             'stimulates': 1,\n",
       "             'thriving': 2,\n",
       "             'variety': 18,\n",
       "             'rapid': 14,\n",
       "             'benefits': 41,\n",
       "             'important': 94,\n",
       "             'vibrant': 3,\n",
       "             'breakthroughs': 4,\n",
       "             'fairly': 1,\n",
       "             'progress': 12,\n",
       "             'astounding': 1,\n",
       "             'integral': 5,\n",
       "             'promising': 6,\n",
       "             'revolutionize': 11,\n",
       "             'profound': 13,\n",
       "             'poised': 3,\n",
       "             'intricate': 1,\n",
       "             'faster': 20,\n",
       "             'seamless': 12,\n",
       "             'hallmark': 1,\n",
       "             'engaging': 8,\n",
       "             'monumental': 1,\n",
       "             'work': 153,\n",
       "             'leading': 26,\n",
       "             'sensitive': 13,\n",
       "             'paramount': 2,\n",
       "             'robust': 9,\n",
       "             'protect': 25,\n",
       "             'ethical': 9,\n",
       "             'noteworthy': 1,\n",
       "             'invaluable': 2,\n",
       "             'enhance': 9,\n",
       "             'valuable': 11,\n",
       "             'promise': 1,\n",
       "             'striking': 3,\n",
       "             'right': 44,\n",
       "             'innovative': 11,\n",
       "             'unquestionably': 1,\n",
       "             'gain': 8,\n",
       "             'astonishing': 1,\n",
       "             'simpler': 1,\n",
       "             'advantage': 12,\n",
       "             'sophisticated': 13,\n",
       "             'patient': 51,\n",
       "             'freedoms': 1,\n",
       "             'encourage': 7,\n",
       "             'diplomatic': 1,\n",
       "             'well-being': 8,\n",
       "             'confidence': 16,\n",
       "             'faith': 2,\n",
       "             'promises': 8,\n",
       "             'supports': 2,\n",
       "             'trust': 15,\n",
       "             'state-of-the-art': 1,\n",
       "             'encouraging': 4,\n",
       "             'accessible': 20,\n",
       "             'interests': 6,\n",
       "             'personalized': 9,\n",
       "             'favour': 3,\n",
       "             'talent': 7,\n",
       "             'available': 37,\n",
       "             'distinction': 2,\n",
       "             'positive': 23,\n",
       "             'enrich': 1,\n",
       "             'revolutionized': 2,\n",
       "             'convenience': 14,\n",
       "             'captivating': 3,\n",
       "             'dazzling': 1,\n",
       "             'modern': 16,\n",
       "             'dawn': 2,\n",
       "             'intriguing': 1,\n",
       "             'remarkable': 4,\n",
       "             'led': 25,\n",
       "             'modest': 1,\n",
       "             'treasure': 1,\n",
       "             'empowerment': 2,\n",
       "             'fertile': 1,\n",
       "             'flourish': 3,\n",
       "             'steady': 4,\n",
       "             'originality': 1,\n",
       "             'creative': 17,\n",
       "             'prowess': 1,\n",
       "             'thrilling': 2,\n",
       "             'heartwarming': 1,\n",
       "             'renaissance': 2,\n",
       "             'expansive': 1,\n",
       "             'supreme': 2,\n",
       "             'unparalleled': 3,\n",
       "             'recommendations': 9,\n",
       "             'commitment': 2,\n",
       "             'formidable': 2,\n",
       "             'enhanced': 6,\n",
       "             'acclaimed': 1,\n",
       "             'popular': 20,\n",
       "             'empower': 4,\n",
       "             'appeal': 1,\n",
       "             'gratification': 2,\n",
       "             'talents': 1,\n",
       "             'refine': 2,\n",
       "             'freedom': 2,\n",
       "             'effortlessly': 2,\n",
       "             'elevate': 1,\n",
       "             'benefit': 17,\n",
       "             'memorable': 2,\n",
       "             'champion': 2,\n",
       "             'strong': 13,\n",
       "             'success': 10,\n",
       "             'flexible': 5,\n",
       "             'perfect': 15,\n",
       "             'easier': 15,\n",
       "             'well-known': 4,\n",
       "             'leads': 17,\n",
       "             'attractive': 3,\n",
       "             'affordable': 11,\n",
       "             'support': 45,\n",
       "             'ideal': 11,\n",
       "             'traction': 4,\n",
       "             'genuine': 1,\n",
       "             'integrated': 7,\n",
       "             'proactive': 6,\n",
       "             'effectively': 18,\n",
       "             'protection': 14,\n",
       "             'best': 49,\n",
       "             'facilitate': 4,\n",
       "             'precisely': 6,\n",
       "             'loyalty': 1,\n",
       "             'leverage': 4,\n",
       "             'secure': 12,\n",
       "             'willing': 2,\n",
       "             'clear': 16,\n",
       "             'overtake': 1,\n",
       "             'improving': 15,\n",
       "             'efficient': 17,\n",
       "             'reliable': 7,\n",
       "             'prefer': 4,\n",
       "             'fast': 14,\n",
       "             'prominence': 2,\n",
       "             'dominated': 3,\n",
       "             'unmatched': 1,\n",
       "             'boost': 13,\n",
       "             'transparent': 4,\n",
       "             'individualized': 2,\n",
       "             'intelligent': 18,\n",
       "             'adaptive': 1,\n",
       "             'effectiveness': 4,\n",
       "             'dominate': 2,\n",
       "             'dependable': 1,\n",
       "             'immense': 3,\n",
       "             'reasonably': 1,\n",
       "             'autonomous': 7,\n",
       "             'friendly': 1,\n",
       "             'wealthy': 2,\n",
       "             'gained': 4,\n",
       "             'healthy': 9,\n",
       "             'flexibility': 7,\n",
       "             'comfort': 9,\n",
       "             'beneficial': 13,\n",
       "             'appropriate': 6,\n",
       "             'convenient': 15,\n",
       "             'easy': 18,\n",
       "             'continuity': 1,\n",
       "             'accurately': 4,\n",
       "             'suitable': 7,\n",
       "             'useful': 20,\n",
       "             'accurate': 22,\n",
       "             'afford': 4,\n",
       "             'sharp': 5,\n",
       "             'capable': 9,\n",
       "             'interesting': 7,\n",
       "             'safe': 16,\n",
       "             'boom': 5,\n",
       "             'gains': 3,\n",
       "             'prompt': 2,\n",
       "             'safely': 5,\n",
       "             'eased': 4,\n",
       "             'knowledgeable': 3,\n",
       "             'proper': 13,\n",
       "             'willingness': 4,\n",
       "             'recover': 5,\n",
       "             'comfortable': 8,\n",
       "             'wonder': 5,\n",
       "             'loyal': 2,\n",
       "             'accomplished': 3,\n",
       "             'groundbreaking': 2,\n",
       "             'likes': 3,\n",
       "             'optimistic': 6,\n",
       "             'outsmart': 1,\n",
       "             'influential': 2,\n",
       "             'instantly': 4,\n",
       "             'thoughtful': 1,\n",
       "             'improvement': 6,\n",
       "             'surpass': 8,\n",
       "             'effortless': 1,\n",
       "             'enough': 26,\n",
       "             'excitement': 4,\n",
       "             'notably': 4,\n",
       "             'economical': 1,\n",
       "             'stronger': 3,\n",
       "             'gaining': 4,\n",
       "             'supporting': 4,\n",
       "             'well-established': 2,\n",
       "             'correct': 4,\n",
       "             'hottest': 1,\n",
       "             'efficiently': 8,\n",
       "             'powerful': 8,\n",
       "             'attraction': 2,\n",
       "             'successful': 14,\n",
       "             'wonderful': 1,\n",
       "             'appealing': 1,\n",
       "             'happiness': 4,\n",
       "             'flawlessly': 3,\n",
       "             'straightforward': 2,\n",
       "             'reputation': 1,\n",
       "             'worthwhile': 1,\n",
       "             'correctly': 4,\n",
       "             'famous': 4,\n",
       "             'successfully': 4,\n",
       "             'strongest': 1,\n",
       "             'fans': 6,\n",
       "             'satisfying': 4,\n",
       "             'honest': 2,\n",
       "             'positives': 2,\n",
       "             'wisdom': 3,\n",
       "             'entice': 1,\n",
       "             'respect': 2,\n",
       "             'ample': 2,\n",
       "             'competitive': 12,\n",
       "             'dynamic': 8,\n",
       "             'ready': 11,\n",
       "             'miraculously': 1,\n",
       "             'durable': 1,\n",
       "             'exceed': 1,\n",
       "             'fast-paced': 3,\n",
       "             'prodigious': 2,\n",
       "             'portable': 1,\n",
       "             'exceeds': 1,\n",
       "             'electrify': 1,\n",
       "             'ambitious': 3,\n",
       "             'promised': 1,\n",
       "             'friendliness': 1,\n",
       "             'cheaper': 8,\n",
       "             'lighter': 1,\n",
       "             'stability': 3,\n",
       "             'optimal': 4,\n",
       "             'superior': 2,\n",
       "             'fantastic': 2,\n",
       "             'stable': 1,\n",
       "             'outshine': 1,\n",
       "             'self-sufficient': 3,\n",
       "             'usable': 1,\n",
       "             'worth': 10,\n",
       "             'loved': 6,\n",
       "             'wellbeing': 2,\n",
       "             'renowned': 1,\n",
       "             'outperform': 3,\n",
       "             'backbone': 1,\n",
       "             'works': 10,\n",
       "             'helped': 18,\n",
       "             'abundance': 1,\n",
       "             'survival': 11,\n",
       "             'outperformed': 1,\n",
       "             'capability': 6,\n",
       "             'delicate': 1,\n",
       "             'concise': 2,\n",
       "             'timely': 6,\n",
       "             'rich': 2,\n",
       "             'impressive': 3,\n",
       "             'enticing': 2,\n",
       "             'impeccable': 2,\n",
       "             'mature': 2,\n",
       "             'fascination': 1,\n",
       "             'wonders': 1,\n",
       "             'admiration': 1,\n",
       "             'pure': 1,\n",
       "             'supremacy': 2,\n",
       "             'defeated': 4,\n",
       "             'skilled': 5,\n",
       "             'perfection': 8,\n",
       "             'properly': 5,\n",
       "             'proves': 2,\n",
       "             'soft': 5,\n",
       "             'calm': 2,\n",
       "             'excel': 3,\n",
       "             'distinguished': 1,\n",
       "             'reward': 1,\n",
       "             'headway': 1,\n",
       "             'simplify': 1,\n",
       "             'speedy': 1,\n",
       "             'endorsing': 1,\n",
       "             'proficient': 2,\n",
       "             'unbiased': 2,\n",
       "             'sufficient': 5,\n",
       "             'satisfy': 4,\n",
       "             'productively': 1,\n",
       "             'soothe': 1,\n",
       "             'complement': 1,\n",
       "             'enhancement': 2,\n",
       "             'wins': 2,\n",
       "             'recommend': 5,\n",
       "             'smarter': 8,\n",
       "             'super': 1,\n",
       "             'happy': 12,\n",
       "             'fascinating': 2,\n",
       "             'famously': 1,\n",
       "             'fortune': 2,\n",
       "             'winning': 2,\n",
       "             'talented': 1,\n",
       "             'advocate': 3,\n",
       "             'tougher': 2,\n",
       "             'undisputed': 1,\n",
       "             'breakthrough': 4,\n",
       "             'helping': 12,\n",
       "             'victory': 1,\n",
       "             'resounding': 1,\n",
       "             'tough': 9,\n",
       "             'abundant': 1,\n",
       "             'clearly': 5,\n",
       "             'educated': 4,\n",
       "             'perfectly': 1,\n",
       "             'silent': 3,\n",
       "             'peacefully': 2,\n",
       "             'empathy': 5,\n",
       "             'qualified': 3,\n",
       "             'enrichment': 1,\n",
       "             'toughest': 1,\n",
       "             'intuitive': 1,\n",
       "             'favorite': 1,\n",
       "             'prosperity': 1,\n",
       "             'fulfillment': 2,\n",
       "             'excellent': 5,\n",
       "             'awesome': 3,\n",
       "             'wise': 1,\n",
       "             'recovery': 3,\n",
       "             'adequate': 7,\n",
       "             'cleverly': 2,\n",
       "             'clean': 4,\n",
       "             'recommended': 2,\n",
       "             'savvy': 1,\n",
       "             'diligent': 1,\n",
       "             'simplest': 2,\n",
       "             'ease': 3,\n",
       "             'user-friendly': 1,\n",
       "             'simplifying': 2,\n",
       "             'flawless': 1,\n",
       "             'realistic': 2,\n",
       "             'humane': 1,\n",
       "             'fancy': 1,\n",
       "             'peaceful': 1,\n",
       "             'top-notch': 1,\n",
       "             'advantageous': 2,\n",
       "             'consistently': 2,\n",
       "             'meaningful': 4,\n",
       "             'clarity': 2,\n",
       "             'defeats': 1,\n",
       "             'master': 3,\n",
       "             'hot': 1,\n",
       "             'fairness': 1,\n",
       "             'comprehensive': 4,\n",
       "             'lean': 1,\n",
       "             'agile': 3,\n",
       "             'motivated': 1,\n",
       "             'unlimited': 1,\n",
       "             'irreplaceable': 1,\n",
       "             'lucrative': 1,\n",
       "             'pretty': 4,\n",
       "             'golden': 1,\n",
       "             'worked': 2,\n",
       "             'reasonable': 2,\n",
       "             'phenomenal': 1,\n",
       "             'incredibly': 1,\n",
       "             'brilliant': 2,\n",
       "             'fine': 2,\n",
       "             'liking': 1,\n",
       "             'inspire': 1,\n",
       "             'endorse': 1,\n",
       "             'fame': 2,\n",
       "             'exceeding': 1,\n",
       "             'sustainability': 5,\n",
       "             'solid': 1,\n",
       "             'goodwill': 1,\n",
       "             'spacious': 1,\n",
       "             'beauty': 4,\n",
       "             'cure': 6,\n",
       "             'fashionable': 1,\n",
       "             'impress': 1,\n",
       "             'defeat': 2,\n",
       "             'enjoying': 2,\n",
       "             'smoother': 1,\n",
       "             'readable': 1,\n",
       "             'adaptable': 5,\n",
       "             'streamlined': 2,\n",
       "             'consistent': 2,\n",
       "             'exciting': 2,\n",
       "             'accomplish': 3,\n",
       "             'exceptionally': 1,\n",
       "             'amazing': 2,\n",
       "             'proud': 1,\n",
       "             'swift': 1,\n",
       "             'irresistible': 1,\n",
       "             'amenable': 1,\n",
       "             'satisfied': 9,\n",
       "             'trusted': 3,\n",
       "             'worthy': 1,\n",
       "             'satisfies': 1,\n",
       "             'masters': 3,\n",
       "             'enjoy': 1,\n",
       "             'prominent': 2,\n",
       "             'enjoys': 1,\n",
       "             'joy': 1,\n",
       "             'harmony': 1,\n",
       "             'love': 2,\n",
       "             'feat': 1,\n",
       "             'enhances': 1,\n",
       "             'pleasure': 2,\n",
       "             'understandable': 2,\n",
       "             'workable': 1,\n",
       "             'relaxed': 2,\n",
       "             'purposeful': 1,\n",
       "             'obsession': 1,\n",
       "             'feasible': 2,\n",
       "             'comfortably': 1,\n",
       "             'upgraded': 3,\n",
       "             'famed': 1,\n",
       "             'enterprising': 1,\n",
       "             'fervent': 1,\n",
       "             'tantalizing': 1,\n",
       "             'luxury': 4,\n",
       "             'thrive': 1,\n",
       "             'win': 3,\n",
       "             'loves': 1,\n",
       "             'recommendation': 1,\n",
       "             'happier': 1,\n",
       "             'helpful': 4,\n",
       "             'warm': 1,\n",
       "             'ardent': 1,\n",
       "             'prolific': 1,\n",
       "             'finest': 1,\n",
       "             'virtue': 1,\n",
       "             'openness': 1,\n",
       "             'humble': 1,\n",
       "             'visionary': 5,\n",
       "             'rewarding': 1,\n",
       "             'unconditional': 1,\n",
       "             'gold': 1,\n",
       "             'succeed': 1,\n",
       "             'passionate': 2,\n",
       "             'inspiring': 3,\n",
       "             'enthusiastic': 1,\n",
       "             'patience': 1,\n",
       "             'industrious': 1,\n",
       "             'smoothly': 1,\n",
       "             'booming': 2,\n",
       "             'awe': 1,\n",
       "             'energy-efficient': 2,\n",
       "             'agility': 5,\n",
       "             'rectifying': 2,\n",
       "             'sturdy': 1,\n",
       "             'well-balanced': 1,\n",
       "             'polished': 1,\n",
       "             'unforgettable': 1,\n",
       "             'brighter': 1,\n",
       "             'privileged': 2,\n",
       "             'encouragement': 1,\n",
       "             'contribution': 2,\n",
       "             'relief': 8,\n",
       "             'dedicated': 1,\n",
       "             'compliant': 1,\n",
       "             'refund': 1,\n",
       "             'smartest': 1,\n",
       "             'long-lasting': 4,\n",
       "             'awards': 1,\n",
       "             'elite': 1,\n",
       "             'winners': 2,\n",
       "             'premier': 1,\n",
       "             'bliss': 1,\n",
       "             'miracle': 1,\n",
       "             'noble': 1,\n",
       "             'flourishing': 1,\n",
       "             'gloriously': 1,\n",
       "             'heal': 1,\n",
       "             'restructured': 1,\n",
       "             'achievements': 2,\n",
       "             'foresight': 2,\n",
       "             'luck': 1,\n",
       "             'reliably': 1,\n",
       "             'pleasing': 1,\n",
       "             'prudent': 1,\n",
       "             'authentic': 1,\n",
       "             'bloom': 1,\n",
       "             'luxurious': 2,\n",
       "             'reform': 1,\n",
       "             'clever': 1,\n",
       "             'entertaining': 1,\n",
       "             'excited': 1,\n",
       "             'marvelous': 1,\n",
       "             'award': 1,\n",
       "             'fanfare': 1,\n",
       "             'refreshed': 1,\n",
       "             'honored': 1,\n",
       "             'remedy': 1,\n",
       "             'manageable': 1,\n",
       "             'welcome': 1,\n",
       "             'subsidizing': 1,\n",
       "             'approval': 1,\n",
       "             'festive': 1,\n",
       "             'solidarity': 2,\n",
       "             'grace': 1,\n",
       "             'grateful': 1,\n",
       "             'thankful': 1,\n",
       "             'dumbfounded': 1,\n",
       "             'responsibly': 1,\n",
       "             'balanced': 2,\n",
       "             'easiest': 1,\n",
       "             'peace': 1,\n",
       "             'clearer': 1,\n",
       "             'gifted': 1,\n",
       "             'stupendous': 1,\n",
       "             'nurturing': 1,\n",
       "             'trump': 1,\n",
       "             'low-cost': 1,\n",
       "             'unity': 1,\n",
       "             'coherent': 1,\n",
       "             'accolades': 1,\n",
       "             'tender': 1,\n",
       "             'acumen': 1,\n",
       "             'passion': 1,\n",
       "             'pros': 1,\n",
       "             'protective': 1,\n",
       "             'glimmer': 1,\n",
       "             'restored': 1,\n",
       "             'trustworthy': 1,\n",
       "             'liked': 1,\n",
       "             'altruistic': 1,\n",
       "             'resilient': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1de8292e-e6b6-4569-ad74-52d31950aa96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'harmful': 13,\n",
       "             'congestion': 3,\n",
       "             'garbage': 2,\n",
       "             'waste': 13,\n",
       "             'disadvantages': 3,\n",
       "             'difficulties': 6,\n",
       "             'issues': 36,\n",
       "             'deplete': 1,\n",
       "             'harm': 6,\n",
       "             'worsen': 5,\n",
       "             'inequities': 4,\n",
       "             'problems': 61,\n",
       "             'negative': 21,\n",
       "             'unexpected': 7,\n",
       "             'pervasive': 2,\n",
       "             'irregular': 1,\n",
       "             'drought': 2,\n",
       "             'impossible': 10,\n",
       "             'unusually': 1,\n",
       "             'loss': 24,\n",
       "             'difficult': 21,\n",
       "             'concerns': 20,\n",
       "             'strike': 1,\n",
       "             'worries': 4,\n",
       "             'struggle': 8,\n",
       "             'limited': 21,\n",
       "             'insatiable': 2,\n",
       "             'lag': 2,\n",
       "             'latency': 4,\n",
       "             'obsolete': 7,\n",
       "             'unimaginable': 5,\n",
       "             'threats': 30,\n",
       "             'complex': 21,\n",
       "             'inequitable': 1,\n",
       "             'risk': 50,\n",
       "             'failure': 8,\n",
       "             'strained': 2,\n",
       "             'inequalities': 4,\n",
       "             'vulnerable': 15,\n",
       "             'sinister': 1,\n",
       "             'worrisome': 1,\n",
       "             'issue': 11,\n",
       "             'dangerous': 9,\n",
       "             'problem': 43,\n",
       "             'illicit': 2,\n",
       "             'attacks': 14,\n",
       "             'critical': 33,\n",
       "             'attack': 8,\n",
       "             'flaws': 1,\n",
       "             'steal': 6,\n",
       "             'devastating': 7,\n",
       "             'susceptible': 6,\n",
       "             'panic': 4,\n",
       "             'instability': 1,\n",
       "             'downturns': 1,\n",
       "             'disruption': 13,\n",
       "             'lost': 14,\n",
       "             'wreak': 1,\n",
       "             'havoc': 7,\n",
       "             'crashing': 1,\n",
       "             'weaknesses': 3,\n",
       "             'disastrous': 2,\n",
       "             'upsetting': 1,\n",
       "             'turmoil': 2,\n",
       "             'embroiled': 1,\n",
       "             'manipulation': 3,\n",
       "             'sabotage': 1,\n",
       "             'disrupt': 5,\n",
       "             'undermine': 2,\n",
       "             'impair': 1,\n",
       "             'hacks': 2,\n",
       "             'hurt': 3,\n",
       "             'erosion': 3,\n",
       "             'degradation': 2,\n",
       "             'extortion': 2,\n",
       "             'interfere': 1,\n",
       "             'exacerbate': 3,\n",
       "             'tense': 1,\n",
       "             'distrust': 3,\n",
       "             'strict': 4,\n",
       "             'intense': 1,\n",
       "             'threat': 31,\n",
       "             'unstable': 2,\n",
       "             'toll': 8,\n",
       "             'stress': 13,\n",
       "             'worry': 16,\n",
       "             'lack': 21,\n",
       "             'gloomy': 1,\n",
       "             'lie': 6,\n",
       "             'criminal': 3,\n",
       "             'risks': 18,\n",
       "             'shadowy': 1,\n",
       "             'lose': 10,\n",
       "             'wane': 1,\n",
       "             'blurring': 2,\n",
       "             'challenging': 9,\n",
       "             'break': 7,\n",
       "             'blur': 4,\n",
       "             'mundane': 2,\n",
       "             'thirst': 2,\n",
       "             'rigid': 5,\n",
       "             'broken': 3,\n",
       "             'limitations': 13,\n",
       "             'limit': 7,\n",
       "             'lesser-known': 2,\n",
       "             'fiction': 5,\n",
       "             'breaks': 1,\n",
       "             'fatigue': 1,\n",
       "             'overwhelmed': 5,\n",
       "             'decline': 17,\n",
       "             'inappropriate': 1,\n",
       "             'fraud': 10,\n",
       "             'weak': 2,\n",
       "             'error': 3,\n",
       "             'errors': 5,\n",
       "             'suffering': 6,\n",
       "             'illegal': 8,\n",
       "             'crime': 4,\n",
       "             'leak': 1,\n",
       "             'lure': 3,\n",
       "             'rival': 2,\n",
       "             'exploit': 10,\n",
       "             'scams': 1,\n",
       "             'inevitably': 3,\n",
       "             'severe': 9,\n",
       "             'losses': 13,\n",
       "             'concern': 18,\n",
       "             'rogue': 2,\n",
       "             'retaliatory': 1,\n",
       "             'aggression': 2,\n",
       "             'hinder': 3,\n",
       "             'slow': 2,\n",
       "             'delaying': 1,\n",
       "             'fall': 18,\n",
       "             'anxiety': 9,\n",
       "             'fear': 29,\n",
       "             'manipulate': 3,\n",
       "             'discord': 1,\n",
       "             'daunting': 2,\n",
       "             'prosecute': 2,\n",
       "             'detrimental': 2,\n",
       "             'menace': 2,\n",
       "             'damages': 2,\n",
       "             'difficulty': 5,\n",
       "             'overstated': 1,\n",
       "             'damage': 5,\n",
       "             'hostile': 2,\n",
       "             'death': 10,\n",
       "             'intrusion': 1,\n",
       "             'deter': 1,\n",
       "             'stolen': 6,\n",
       "             'uncertain': 2,\n",
       "             'isolated': 1,\n",
       "             'falling': 3,\n",
       "             'offensive': 1,\n",
       "             'explosive': 1,\n",
       "             'upheaval': 1,\n",
       "             'illness': 6,\n",
       "             'unequal': 1,\n",
       "             'lacking': 3,\n",
       "             'expensive': 9,\n",
       "             'costly': 6,\n",
       "             'chronic': 13,\n",
       "             'symptoms': 5,\n",
       "             'strain': 5,\n",
       "             'poor': 4,\n",
       "             'unnecessary': 2,\n",
       "             'redundant': 2,\n",
       "             'drawbacks': 3,\n",
       "             'unable': 8,\n",
       "             'virus': 46,\n",
       "             'notorious': 1,\n",
       "             'stringent': 3,\n",
       "             'drastically': 7,\n",
       "             'adversity': 3,\n",
       "             'restricted': 8,\n",
       "             'needy': 2,\n",
       "             'nefarious': 1,\n",
       "             'cripple': 1,\n",
       "             'chaos': 3,\n",
       "             'confusion': 3,\n",
       "             'insecure': 1,\n",
       "             'scared': 6,\n",
       "             'extort': 1,\n",
       "             'emergency': 9,\n",
       "             'obscure': 2,\n",
       "             'nightmare': 2,\n",
       "             'pain': 7,\n",
       "             'frustration': 2,\n",
       "             'wasting': 2,\n",
       "             'useless': 3,\n",
       "             'tragedy': 2,\n",
       "             'breaking': 4,\n",
       "             'sick': 4,\n",
       "             'urgent': 5,\n",
       "             'hard': 25,\n",
       "             'struck': 4,\n",
       "             'shortage': 5,\n",
       "             'crisis': 36,\n",
       "             'endanger': 1,\n",
       "             'ignore': 3,\n",
       "             'cumbersome': 1,\n",
       "             'absence': 6,\n",
       "             'restriction': 2,\n",
       "             'obstacle': 2,\n",
       "             'hype': 3,\n",
       "             'epidemic': 6,\n",
       "             'insecurity': 1,\n",
       "             'excessive': 5,\n",
       "             'bad': 10,\n",
       "             'criticism': 3,\n",
       "             'inextricable': 1,\n",
       "             'debatable': 1,\n",
       "             'wrong': 11,\n",
       "             'worse': 10,\n",
       "             'negatives': 1,\n",
       "             'infamous': 1,\n",
       "             'adverse': 8,\n",
       "             'unrealistic': 2,\n",
       "             'unhealthy': 2,\n",
       "             'disturbed': 2,\n",
       "             'vicious': 2,\n",
       "             'futile': 1,\n",
       "             'falsely': 1,\n",
       "             'false': 2,\n",
       "             'conflicted': 1,\n",
       "             'evils': 1,\n",
       "             'criticized': 1,\n",
       "             'gimmicks': 1,\n",
       "             'junk': 2,\n",
       "             'hamper': 1,\n",
       "             'failed': 3,\n",
       "             'worsening': 2,\n",
       "             'erratic': 1,\n",
       "             'pigs': 1,\n",
       "             'infections': 3,\n",
       "             'senile': 1,\n",
       "             'passive': 1,\n",
       "             'crude': 4,\n",
       "             'hasty': 1,\n",
       "             'adamant': 1,\n",
       "             'worst': 4,\n",
       "             'harmed': 1,\n",
       "             'congested': 1,\n",
       "             'uproar': 1,\n",
       "             'redundancy': 2,\n",
       "             'fragile': 3,\n",
       "             'unipolar': 1,\n",
       "             'lies': 9,\n",
       "             'lagging': 2,\n",
       "             'spilling': 1,\n",
       "             'toxic': 2,\n",
       "             'improper': 1,\n",
       "             'dumped': 2,\n",
       "             'illegally': 1,\n",
       "             'contamination': 2,\n",
       "             'afraid': 9,\n",
       "             'unwelcome': 1,\n",
       "             'declining': 1,\n",
       "             'conservative': 1,\n",
       "             'slow-moving': 1,\n",
       "             'die': 2,\n",
       "             'sickness': 1,\n",
       "             'disorder': 12,\n",
       "             'suffer': 4,\n",
       "             'burden': 4,\n",
       "             'trouble': 2,\n",
       "             'sore': 1,\n",
       "             'kills': 1,\n",
       "             'outbreak': 22,\n",
       "             'invisible': 2,\n",
       "             'unprepared': 1,\n",
       "             'oblivious': 1,\n",
       "             'alarming': 2,\n",
       "             'infected': 5,\n",
       "             'cloud': 13,\n",
       "             'cancer': 8,\n",
       "             'fatal': 2,\n",
       "             'syndrome': 3,\n",
       "             'abnormal': 1,\n",
       "             'regression': 2,\n",
       "             'distress': 2,\n",
       "             'fuzzy': 2,\n",
       "             'dust': 1,\n",
       "             'appalled': 1,\n",
       "             'unravel': 1,\n",
       "             'greed': 1,\n",
       "             'unskilled': 3,\n",
       "             'oppose': 1,\n",
       "             'fearful': 6,\n",
       "             'scary': 3,\n",
       "             'drowning': 2,\n",
       "             'time-consuming': 5,\n",
       "             'crushed': 1,\n",
       "             'inevitable': 4,\n",
       "             'risky': 2,\n",
       "             'unethical': 3,\n",
       "             'buckle': 1,\n",
       "             'complicated': 3,\n",
       "             'concerned': 6,\n",
       "             'misfortune': 1,\n",
       "             'stranger': 2,\n",
       "             'misrepresentation': 1,\n",
       "             'danger': 3,\n",
       "             'fraudulent': 1,\n",
       "             'suspicious': 2,\n",
       "             'refuse': 1,\n",
       "             'wasteful': 1,\n",
       "             'chore': 1,\n",
       "             'dread': 1,\n",
       "             'denying': 1,\n",
       "             'unrest': 2,\n",
       "             'repetitive': 11,\n",
       "             'burn': 1,\n",
       "             'primitive': 1,\n",
       "             'conflicts': 2,\n",
       "             'falter': 1,\n",
       "             'implication': 1,\n",
       "             'disruptive': 5,\n",
       "             'exhausted': 1,\n",
       "             'ironic': 1,\n",
       "             'unfortunately': 1,\n",
       "             'disaster': 7,\n",
       "             'tired': 1,\n",
       "             'unpredictable': 2,\n",
       "             'moot': 1,\n",
       "             'impractical': 1,\n",
       "             'catastrophic': 2,\n",
       "             'crash': 2,\n",
       "             'mess': 1,\n",
       "             'incapable': 2,\n",
       "             'vice': 3,\n",
       "             'drones': 2,\n",
       "             'dehumanize': 1,\n",
       "             'unknown': 5,\n",
       "             'divergent': 1,\n",
       "             'downside': 5,\n",
       "             'badly': 1,\n",
       "             'sloppy': 1,\n",
       "             'smudged': 1,\n",
       "             'struggling': 4,\n",
       "             'fears': 8,\n",
       "             'fatalistic': 1,\n",
       "             'runaway': 1,\n",
       "             'limitation': 2,\n",
       "             'bias': 9,\n",
       "             'biases': 4,\n",
       "             'fallacy': 2,\n",
       "             'doubt': 4,\n",
       "             'rude': 1,\n",
       "             'rejected': 1,\n",
       "             'prejudice': 1,\n",
       "             'discontinued': 1,\n",
       "             'controversial': 3,\n",
       "             'limits': 2,\n",
       "             'scramble': 1,\n",
       "             'sneaky': 2,\n",
       "             'malicious': 2,\n",
       "             'infection': 6,\n",
       "             'unreadable': 2,\n",
       "             'evil': 1,\n",
       "             'unfamiliar': 2,\n",
       "             'hack': 1,\n",
       "             'stuck': 3,\n",
       "             'disable': 2,\n",
       "             'idle': 1,\n",
       "             'flagging': 1,\n",
       "             'inconvenience': 1,\n",
       "             'delay': 4,\n",
       "             'joke': 4,\n",
       "             'foe': 1,\n",
       "             'destruction': 4,\n",
       "             'plot': 1,\n",
       "             'jobless': 1,\n",
       "             'losing': 4,\n",
       "             'recession': 8,\n",
       "             'fuss': 1,\n",
       "             'interference': 1,\n",
       "             'biased': 4,\n",
       "             'unclear': 2,\n",
       "             'impending': 1,\n",
       "             'ambiguous': 1,\n",
       "             'burning': 1,\n",
       "             'controversy': 1,\n",
       "             'displace': 2,\n",
       "             'noise': 9,\n",
       "             'repress': 1,\n",
       "             'doomsday': 1,\n",
       "             'irresponsible': 1,\n",
       "             'pessimistic': 1,\n",
       "             'warned': 1,\n",
       "             'unhappy': 3,\n",
       "             'dissonance': 2,\n",
       "             'displaced': 1,\n",
       "             'distraction': 2,\n",
       "             'threaten': 1,\n",
       "             'terrible': 1,\n",
       "             'volatility': 1,\n",
       "             'falls': 1,\n",
       "             'struggled': 1,\n",
       "             'jaded': 1,\n",
       "             'horrendous': 1,\n",
       "             'imposing': 2,\n",
       "             'unwanted': 1,\n",
       "             'rail': 1,\n",
       "             'imposition': 2,\n",
       "             'short-lived': 1,\n",
       "             'lying': 1,\n",
       "             'degrading': 1,\n",
       "             'careless': 1,\n",
       "             'desert': 2,\n",
       "             'warning': 1,\n",
       "             'unusual': 1,\n",
       "             'insufficient': 1,\n",
       "             'misconception': 1,\n",
       "             'fool': 1,\n",
       "             'trivial': 1,\n",
       "             'hectic': 2,\n",
       "             'fallen': 1,\n",
       "             'sharply': 1,\n",
       "             'slowed': 1,\n",
       "             'messy': 1,\n",
       "             'glitches': 1,\n",
       "             'loneliness': 9,\n",
       "             'lonely': 4,\n",
       "             'isolation': 7,\n",
       "             'loophole': 1,\n",
       "             'crack': 2,\n",
       "             'weep': 1,\n",
       "             'depression': 8,\n",
       "             'anger': 2,\n",
       "             'sadness': 2,\n",
       "             'trauma': 2,\n",
       "             'sad': 1,\n",
       "             'loses': 1,\n",
       "             'neglected': 2,\n",
       "             'monstrous': 1,\n",
       "             'grim': 3,\n",
       "             'sadly': 1,\n",
       "             'scream': 1,\n",
       "             'vain': 1,\n",
       "             'writhe': 1,\n",
       "             'slowly': 2,\n",
       "             'creeps': 1,\n",
       "             'succumb': 1,\n",
       "             'subjected': 3,\n",
       "             'inhumane': 1,\n",
       "             'discomfort': 1,\n",
       "             'cruelty': 1,\n",
       "             'excruciating': 1,\n",
       "             'mysterious': 1,\n",
       "             'incomplete': 1,\n",
       "             'denies': 1,\n",
       "             'strictly': 1,\n",
       "             'guilt': 1,\n",
       "             'mystery': 2,\n",
       "             'object': 1,\n",
       "             'intermittent': 1,\n",
       "             'lacks': 2,\n",
       "             'devoid': 2,\n",
       "             'injury': 1,\n",
       "             'irreversible': 1,\n",
       "             'involuntary': 1,\n",
       "             'dead': 2,\n",
       "             'damaging': 2,\n",
       "             'petrified': 1,\n",
       "             'subdued': 1,\n",
       "             'far-fetched': 1,\n",
       "             'pest': 1,\n",
       "             'pricey': 1,\n",
       "             'irrepressible': 1,\n",
       "             'ruthlessly': 1,\n",
       "             'apathetic': 1,\n",
       "             'catastrophe': 1,\n",
       "             'self-destructive': 1,\n",
       "             'weakening': 3,\n",
       "             'feverish': 1,\n",
       "             'unthinkable': 1,\n",
       "             'remorse': 1,\n",
       "             'lame': 1,\n",
       "             'poorly': 1,\n",
       "             'spewing': 1,\n",
       "             'unreachable': 1,\n",
       "             'mistakes': 13,\n",
       "             'worrier': 2,\n",
       "             'ashamed': 4,\n",
       "             'indecisive': 2,\n",
       "             'anxious': 4,\n",
       "             'loud': 2,\n",
       "             'dark': 2,\n",
       "             'grievances': 2,\n",
       "             'trap': 2,\n",
       "             'blind': 4,\n",
       "             'mistake': 4,\n",
       "             'worried': 3,\n",
       "             'illogical': 2,\n",
       "             'cloudy': 2,\n",
       "             'panicked': 2,\n",
       "             'frenzied': 2,\n",
       "             'excessively': 2,\n",
       "             'shortcomings': 1,\n",
       "             'lied': 1,\n",
       "             'dissatisfied': 1,\n",
       "             'dissatisfaction': 3,\n",
       "             'enemy': 4,\n",
       "             'restless': 1,\n",
       "             'collapse': 2,\n",
       "             'fell': 2,\n",
       "             'fail': 2,\n",
       "             'bankrupt': 3,\n",
       "             'outcry': 1,\n",
       "             'queer': 2,\n",
       "             'discrimination': 3,\n",
       "             'stereotypical': 1,\n",
       "             'penalty': 1,\n",
       "             'denied': 2,\n",
       "             'prejudices': 1,\n",
       "             'harassment': 2,\n",
       "             'gross': 1,\n",
       "             'fallout': 2,\n",
       "             'uneven': 1,\n",
       "             'protests': 1,\n",
       "             'brutality': 1,\n",
       "             'shock': 7,\n",
       "             'harsh': 1,\n",
       "             'interruptions': 1,\n",
       "             'harassed': 1,\n",
       "             'drawback': 1,\n",
       "             'incompatible': 1,\n",
       "             'breakdown': 1,\n",
       "             'ineligible': 1,\n",
       "             'reluctant': 2,\n",
       "             'sever': 1,\n",
       "             'curse': 1,\n",
       "             'unlikely': 5,\n",
       "             'bleak': 1,\n",
       "             'tentatively': 1,\n",
       "             'delayed': 4,\n",
       "             'second-tier': 1,\n",
       "             'clash': 1,\n",
       "             'blow': 1,\n",
       "             'strife': 1,\n",
       "             'predicament': 3,\n",
       "             'moribund': 1,\n",
       "             'exorbitantly': 1,\n",
       "             'hefty': 3,\n",
       "             'strenuous': 1,\n",
       "             'confined': 1,\n",
       "             'rumors': 1,\n",
       "             'ignorance': 2,\n",
       "             'exorbitant': 1,\n",
       "             'obscurity': 1,\n",
       "             'abolish': 1,\n",
       "             'stark': 2,\n",
       "             'irrelevant': 1,\n",
       "             'inoperable': 1,\n",
       "             'resigned': 1,\n",
       "             'unfortunate': 2,\n",
       "             'contentious': 1,\n",
       "             'impaired': 1,\n",
       "             'severity': 1,\n",
       "             'deterioration': 2,\n",
       "             'inactive': 1,\n",
       "             'headaches': 1,\n",
       "             'vomiting': 1,\n",
       "             'sufferers': 1,\n",
       "             'fault': 1,\n",
       "             'complaints': 3,\n",
       "             'blister': 1,\n",
       "             'friction': 2,\n",
       "             'fracture': 2,\n",
       "             'sarcastically': 1,\n",
       "             'overweight': 1,\n",
       "             'vomit': 1,\n",
       "             'disturbing': 1,\n",
       "             'fictional': 1,\n",
       "             'aggressive': 1,\n",
       "             'flareups': 1,\n",
       "             'disturb': 1,\n",
       "             'dismal': 1,\n",
       "             'confused': 1,\n",
       "             'complain': 1,\n",
       "             'missed': 1,\n",
       "             'unemployed': 2,\n",
       "             'stalls': 1,\n",
       "             'frightening': 1,\n",
       "             'stall': 3,\n",
       "             'crowded': 1,\n",
       "             'downturn': 2,\n",
       "             'hurts': 2,\n",
       "             'declines': 1,\n",
       "             'debt': 7,\n",
       "             'inability': 1,\n",
       "             'deviation': 1,\n",
       "             'delays': 6,\n",
       "             'treacherous': 1,\n",
       "             'threatening': 1,\n",
       "             'standstill': 1,\n",
       "             'poverty': 3,\n",
       "             'exploitation': 1,\n",
       "             'bore': 1,\n",
       "             'suffered': 1,\n",
       "             'excuse': 1,\n",
       "             'calamities': 2,\n",
       "             'unforgiving': 1,\n",
       "             'fake': 3,\n",
       "             'abuse': 2,\n",
       "             'starvation': 1,\n",
       "             'doom': 1,\n",
       "             'loopholes': 1,\n",
       "             'stagnate': 1,\n",
       "             'reject': 1,\n",
       "             'rejection': 4,\n",
       "             'picky': 1,\n",
       "             'weaken': 1,\n",
       "             'blurred': 1,\n",
       "             'terribly': 1,\n",
       "             'cash-strapped': 1,\n",
       "             'overwhelming': 1,\n",
       "             'illusion': 2,\n",
       "             'paradoxically': 1,\n",
       "             'peculiar': 1,\n",
       "             'trapped': 1,\n",
       "             'froze': 1,\n",
       "             'tarnished': 1,\n",
       "             'drained': 1,\n",
       "             'skinny': 1,\n",
       "             'disturbance': 1,\n",
       "             'sucked': 1,\n",
       "             'insensitive': 1,\n",
       "             'miss': 1,\n",
       "             'numb': 1,\n",
       "             'stern': 1,\n",
       "             'imbalance': 1,\n",
       "             'poison': 1,\n",
       "             'cons': 1,\n",
       "             'impose': 1,\n",
       "             'selfish': 1,\n",
       "             'worn': 1,\n",
       "             'damaged': 1,\n",
       "             'setback': 2,\n",
       "             'restrict': 1,\n",
       "             'kill': 1,\n",
       "             'counterproductive': 1,\n",
       "             'relentless': 1,\n",
       "             'backward': 1,\n",
       "             'weaker': 1,\n",
       "             'sorrow': 1,\n",
       "             'despise': 1,\n",
       "             'torture': 1,\n",
       "             'shunned': 1,\n",
       "             'scratch': 1,\n",
       "             'overlook': 1,\n",
       "             'hatred': 1,\n",
       "             'hated': 1,\n",
       "             'contaminated': 1,\n",
       "             'scam': 1,\n",
       "             'maliciously': 1,\n",
       "             'misleading': 1,\n",
       "             'bored': 1,\n",
       "             'slower': 1,\n",
       "             'traumatic': 1,\n",
       "             'inadequate': 1,\n",
       "             'stigma': 1,\n",
       "             'poorer': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6005043-d5bb-46b3-846b-cc5581558139",
   "metadata": {},
   "source": [
    "### Extract and Save positive and negative words into Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e7f1c57-2898-4662-90a9-effd55d97ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert defaultdicts to regular dictionaries\n",
    "positive_words_dict = dict(positive_words)\n",
    "negative_words_dict = dict(negative_words)\n",
    "\n",
    "# Create DataFrames from dictionaries\n",
    "df_positive = pd.DataFrame({'Word': list(positive_words_dict.keys()), 'Frequency': list(positive_words_dict.values())})\n",
    "df_negative = pd.DataFrame({'Word': list(negative_words_dict.keys()), 'Frequency': list(negative_words_dict.values())})\n",
    "\n",
    "# Save positive and negative words to Excel\n",
    "with pd.ExcelWriter('positive_negative_words.xlsx') as writer:\n",
    "    df_positive.to_excel(writer, sheet_name='Positive', index=False)\n",
    "    df_negative.to_excel(writer, sheet_name='Negative', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4fb57b-8f86-4a10-8fb7-0e62a0c340e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>free</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lead</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>better</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>good</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>intelligence</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>innovation</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>extraordinary</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>smart</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Frequency\n",
       "0           free         10\n",
       "1           like        204\n",
       "2          great         34\n",
       "3           lead         38\n",
       "4         better        113\n",
       "5           good         52\n",
       "6   intelligence        211\n",
       "7     innovation        122\n",
       "8  extraordinary        100\n",
       "9          smart         31"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_excel(r'A:\\MTECH(Data Science)\\NLP\\New folder\\positive_negative_words.xlsx')\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b3d76-4a12-4b97-8a0e-e52b4c5c2061",
   "metadata": {},
   "source": [
    "#### Read the second sheet of the Excel file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68fc77d2-53b5-43b3-93df-2454bd7dce4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>harmful</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>congestion</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>garbage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>waste</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disadvantages</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>difficulties</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>issues</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deplete</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>harm</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>worsen</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Frequency\n",
       "0        harmful         13\n",
       "1     congestion          3\n",
       "2        garbage          2\n",
       "3          waste         13\n",
       "4  disadvantages          3\n",
       "5   difficulties          6\n",
       "6         issues         36\n",
       "7        deplete          1\n",
       "8           harm          6\n",
       "9         worsen          5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the second sheet of the Excel file into a DataFrame\n",
    "df2 = pd.read_excel(r'A:\\MTECH(Data Science)\\NLP\\New folder\\positive_negative_words.xlsx', sheet_name=1)\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "df2.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2678299e-d6fb-4c49-bb37-4edd08410bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sheet For Positive_Words (df1):\n",
      "            Word  Frequency\n",
      "0           free         10\n",
      "1           like        204\n",
      "2          great         34\n",
      "3           lead         38\n",
      "4         better        113\n",
      "5           good         52\n",
      "6   intelligence        211\n",
      "7     innovation        122\n",
      "8  extraordinary        100\n",
      "9          smart         31\n",
      "\n",
      "Second sheet For Nagative_Words (df2):\n",
      "            Word  Frequency\n",
      "0        harmful         13\n",
      "1     congestion          3\n",
      "2        garbage          2\n",
      "3          waste         13\n",
      "4  disadvantages          3\n",
      "5   difficulties          6\n",
      "6         issues         36\n",
      "7        deplete          1\n",
      "8           harm          6\n",
      "9         worsen          5\n"
     ]
    }
   ],
   "source": [
    "# Read both sheets of the Excel file into separate DataFrames\n",
    "df1 = pd.read_excel(r'A:\\MTECH(Data Science)\\NLP\\New folder\\positive_negative_words.xlsx', sheet_name=0)\n",
    "df2 = pd.read_excel(r'A:\\MTECH(Data Science)\\NLP\\New folder\\positive_negative_words.xlsx', sheet_name=1)\n",
    "\n",
    "# Display the first 10 rows of each DataFrame\n",
    "print(\"First sheet For Positive_Words (df1):\")\n",
    "print(df1.head(10))\n",
    "\n",
    "print(\"\\nSecond sheet For Nagative_Words (df2):\")\n",
    "print(df2.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c3da0-bbbe-4de9-851b-d50456069256",
   "metadata": {},
   "source": [
    "# **********************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8cb1bb-6c93-4f39-b7e6-64fd52aac782",
   "metadata": {},
   "source": [
    "## We have to calculate:\n",
    "- 1.\tPOSITIVE SCORE\n",
    "- 2.\tNEGATIVE SCORE\n",
    "- 3.\tPOLARITY SCORE\n",
    "- 4.\tSUBJECTIVITY SCORE\n",
    "- 5.\tAVG SENTENCE LENGTH\n",
    "- 6.\tPERCENTAGE OF COMPLEX WORDS\n",
    "- 7.\tFOG INDEX\n",
    "- 8.\tAVG NUMBER OF WORDS PER SENTENCE\n",
    "- 9.\tCOMPLEX WORD COUNT\n",
    "- 10.\tWORD COUNT\n",
    "- 11.\tSYLLABLE PER WORD\n",
    "- 12.\tPERSONAL PRONOUNS\n",
    "- 13.\tAVG WORD LENGTH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc9e4bf-a28c-4183-80ed-bc78788e1ef3",
   "metadata": {},
   "source": [
    "# *********************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb6df71-2095-4fd0-b264-0270eabd7ff6",
   "metadata": {},
   "source": [
    "#### 1. Positive Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd219bcb-4a38-4d54-8564-32f199e1866f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Score: 3860\n"
     ]
    }
   ],
   "source": [
    "# Calculate the positive score\n",
    "positive_score = df1['Frequency'].sum()\n",
    "\n",
    "print(\"Positive Score:\", positive_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "327286cb-4b83-4754-97fd-9f9f3e78aee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Score: 3860\n"
     ]
    }
   ],
   "source": [
    "# Initialize positive score\n",
    "positive_score = 0\n",
    "\n",
    "# Calculate positive score\n",
    "for index, row in df1.iterrows():\n",
    "    word = row['Word']\n",
    "    if word in positive_words:\n",
    "        positive_score += row['Frequency']  # Assigning a value of +1 for each positive word found\n",
    "\n",
    "# Print the positive score\n",
    "print(\"Positive Score:\", positive_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94652bdb-9480-423d-8b01-3e8aa5fd639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Score: 3860\n"
     ]
    }
   ],
   "source": [
    "# Initialize positive score\n",
    "positive_score = 0\n",
    "\n",
    "# Calculate positive score\n",
    "for index, row in df1.iterrows():\n",
    "    word = row['Word']\n",
    "    if word in positive_words:\n",
    "        positive_score += row['Frequency']  # Assigning a value of +1 for each positive word found\n",
    "\n",
    "# Print the positive score\n",
    "print(\"Positive Score:\", positive_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376b013-5403-4784-817d-fe784db99d83",
   "metadata": {},
   "source": [
    "### 2. Positive Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b887b1a6-6272-4974-94d8-16e9c814c1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Score: 2308\n"
     ]
    }
   ],
   "source": [
    "# Initialize negative score\n",
    "negative_score = 0\n",
    "\n",
    "# Calculate negative score\n",
    "for index, row in df2.iterrows():\n",
    "    word = row['Word']\n",
    "    if word in negative_words:\n",
    "        negative_score -= row['Frequency']  # Assigning a value of -1 for each negative word found\n",
    "\n",
    "# Print the negative score\n",
    "print(\"Negative Score:\", -1 * negative_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c8df9-1e02-43bd-8144-d943faa13c37",
   "metadata": {},
   "source": [
    "### 3. Polarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ef07da8-9ac1-4668-8431-132e0e9a1b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity Score: 1\n"
     ]
    }
   ],
   "source": [
    "# Calculate Polarity Score\n",
    "polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "\n",
    "# Ensure Polarity Score is within range of -1 to +1\n",
    "polarity_score = max(-1, min(1, polarity_score))\n",
    "\n",
    "# Print the Polarity Score\n",
    "print(\"Polarity Score:\", polarity_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29bb421-e31e-4afd-8155-dd1dd6d411a8",
   "metadata": {},
   "source": [
    "### Function to calculate total number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96f1ec18-1d54-4ef4-b378-ee4bced177f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words in DataFrame: 90030\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate total number of words\n",
    "def calculate_total_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Apply the function to calculate total words for each row and then sum them up\n",
    "total_words = df['Text'].apply(lambda x: calculate_total_words(x)).sum()\n",
    "\n",
    "# Print total number of words\n",
    "print(\"Total Words in DataFrame:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdce8bf9-cd0a-4051-820a-b452026e3d82",
   "metadata": {},
   "source": [
    "### 4. Subjectivity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc6d6148-b6aa-4d8b-b37f-9da7eea23c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjectivity Score: 0.017238698211515732\n"
     ]
    }
   ],
   "source": [
    "# Calculate Subjectivity Score\n",
    "subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
    "\n",
    "# Ensure Subjectivity Score is within range of 0 to +1\n",
    "subjectivity_score = max(0, min(1, subjectivity_score))\n",
    "\n",
    "# Print the Subjectivity Score\n",
    "print(\"Subjectivity Score:\", subjectivity_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfae3bda-aeba-4f42-8215-d65378448b48",
   "metadata": {},
   "source": [
    "### Function to calculate total number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea9f9b99-bf72-4985-be53-8afb4dd3591f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences in DataFrame: 5646\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate total number of sentences\n",
    "def calculate_total_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "# Apply the function to calculate total sentences for each row and then sum them up\n",
    "total_sentences = df['Text'].apply(lambda x: calculate_total_sentences(x)).sum()\n",
    "\n",
    "# Print total number of sentences\n",
    "print(\"Total Sentences in DataFrame:\", total_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a30e105-4fd3-4365-b2ac-a46ef961a563",
   "metadata": {},
   "source": [
    "### 5. Average Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e77e628a-a113-4e36-969d-c7bfb7dcf4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.945802337938364"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Average_Sentence_Length = total_words / total_sentences\n",
    "Average_Sentence_Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f485ef-e61d-40df-b7b7-26ae05d2cd44",
   "metadata": {},
   "source": [
    "### Function To Calculate the total number of complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2ecaa4b-fc17-469f-b5cc-f6e5d9922f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:27<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of complex words: 1628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the CMU Pronouncing Dictionary if you haven't already\n",
    "nltk.download('cmudict')\n",
    "# Function to calculate the number of complex words in a text\n",
    "def count_complex_words(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text.lower())  # Convert to lowercase to match CMU dictionary\n",
    "    complex_word_count = 0\n",
    "    d = cmudict.dict()\n",
    "    \n",
    "    # Check each word\n",
    "    for word in words:\n",
    "        # Check if the word is in the CMU dictionary\n",
    "        if word in d:\n",
    "            # Check if the word has more than one syllable\n",
    "            if len([ph for ph in d[word] if any(char.isalpha() for char in ph)]) > 2:\n",
    "                complex_word_count += 1\n",
    "    \n",
    "    return complex_word_count\n",
    "\n",
    "# Apply the function to each row of the DataFrame with tqdm\n",
    "tqdm.pandas()\n",
    "d1 = df['Text'].progress_apply(count_complex_words)\n",
    "\n",
    "# Calculate the total number of complex words\n",
    "total_complex_words = d1.sum()\n",
    "\n",
    "print(\"Total number of complex words:\", total_complex_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dcb7a9-effd-441e-a3cb-10c475c366e7",
   "metadata": {},
   "source": [
    "### 6. Percentage of Complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f78cdd8-4b51-4ca9-9216-e54c9149589b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8082861268466068"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Percentage_of_Complex_words = (total_complex_words / total_words)*100\n",
    "Percentage_of_Complex_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a7b05-28eb-4afa-a241-1f1958319116",
   "metadata": {},
   "source": [
    "### 7. Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "943fbc0f-5010-44e6-9208-31994876f1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.101635385913989"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_words)\n",
    "Fog_Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a1a622-9508-42a4-a934-65920f029af8",
   "metadata": {},
   "source": [
    "### 8. AVG NUMBER OF WORDS PER SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ff1b0ee-e7ee-408f-aedf-ecd19f03b421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     13.758621\n",
      "1     14.926829\n",
      "2     16.295082\n",
      "3     17.928571\n",
      "4     15.534884\n",
      "        ...    \n",
      "95    17.055556\n",
      "96    19.428571\n",
      "97    33.222222\n",
      "98    15.942857\n",
      "99    23.588235\n",
      "Name: Text, Length: 100, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def average_words_per_sentence(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    total_words = 0\n",
    "\n",
    "    # Iterate through each sentence to count words\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        total_words += len(words)\n",
    "\n",
    "    # Calculate average number of words per sentence\n",
    "    if len(sentences) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return total_words / len(sentences)\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "d = df['Text'].apply(average_words_per_sentence)\n",
    "\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847aa1c8-4050-4b4a-a318-66f9bdef5593",
   "metadata": {},
   "source": [
    "### 9. COMPLEX WORD COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6315c2ef-7b25-4a85-8e17-5cef056f4405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 79.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "     ..\n",
      "95    0\n",
      "96    0\n",
      "97    0\n",
      "98    0\n",
      "99    0\n",
      "Name: Text, Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Download CMU Pronouncing Dictionary if not already downloaded\n",
    "nltk.download('cmudict')\n",
    "\n",
    "# Load CMU Pronouncing Dictionary and create a set of words\n",
    "cmu_dict = cmudict.dict()\n",
    "cmu_words_set = set(cmu_dict.keys())\n",
    "\n",
    "def count_complex_words(text):\n",
    "    \"\"\"\n",
    "    Function to count the number of complex words in a given text.\n",
    "    \"\"\"\n",
    "    # Tokenize the text using regular expressions\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    complex_word_count = 0\n",
    "    for word in words:\n",
    "        # Check if word is in the set of CMU words\n",
    "        if word in cmu_words_set:\n",
    "            # Check if any phoneme contains a digit (indicating multiple syllables)\n",
    "            if any(any(char.isdigit() for char in phoneme) for phoneme in cmu_dict[word]):\n",
    "                complex_word_count += 1\n",
    "    \n",
    "    return complex_word_count\n",
    "\n",
    "# Apply the count_complex_words function to each row in the DataFrame with tqdm\n",
    "tqdm.pandas()\n",
    "l = df['Text'].progress_apply(count_complex_words)\n",
    "\n",
    "print(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c443ce-e1fa-43c3-93af-15ed77d58322",
   "metadata": {},
   "source": [
    "### 10. WORD COUNT\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efbac8d1-9948-49b1-921a-75f1a3aa3af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 784\n",
      "0      332\n",
      "1     1007\n",
      "2      818\n",
      "3      822\n",
      "4      562\n",
      "      ... \n",
      "95     803\n",
      "96     679\n",
      "97     244\n",
      "98     464\n",
      "99     681\n",
      "Name: Text, Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Function to count cleaned words in the text\n",
    "def count_cleaned_words(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation marks\n",
    "    words = [word for word in words if word not in string.punctuation]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Count the remaining words\n",
    "    cleaned_word_count = len(words)\n",
    "    \n",
    "    return cleaned_word_count\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "c = df['Text'].apply(count_cleaned_words)\n",
    "\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e93548-b82a-4e32-b0cb-9cc02853de17",
   "metadata": {},
   "source": [
    "### 11. SYLLABLE PER WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42b7d247-e4d1-4e54-b1a4-0b55461b1bbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "1     [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "2     [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "3     [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "4     [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "                            ...                        \n",
      "95    [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "96    [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "97    [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "98    [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "99    [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "Name: Text, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your count_syllables function remains the same as before\n",
    "\n",
    "# Modify count_syllables_in_text to handle words shorter than 4 characters\n",
    "def count_syllables_in_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Initialize a list to store syllable counts\n",
    "    syllable_counts = []\n",
    "    \n",
    "    # Loop through each word\n",
    "    for word in words:\n",
    "        # Count syllables in the word\n",
    "        syllable_count = count_syllables(word)\n",
    "        \n",
    "        # Append the syllable count to the list\n",
    "        syllable_counts.append(syllable_count)\n",
    "    \n",
    "    return syllable_counts\n",
    "\n",
    "\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "e = df['Text'].apply(count_syllables_in_text)\n",
    "\n",
    "print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "711b2fc3-d1a6-445e-bf53-4245933f1ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "1     [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "2     [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "3     [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "4     [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "                            ...                        \n",
      "95    [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "96    [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "97    [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "98    [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "99    [2, 1, 1, 2, 1, 3, 3, 1, 1, 3, 4, 3, 3, 1, 4, ...\n",
      "Name: Text, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to count syllables in a word while handling exceptions\n",
    "def count_syllables(word):\n",
    "    # Remove punctuation marks from the word\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    \n",
    "    # Count vowels\n",
    "    vowels = 'aeiou'\n",
    "    num_vowels = 0\n",
    "    prev_char = ''\n",
    "    \n",
    "    for char in word.lower():\n",
    "        if char in vowels and prev_char not in vowels:\n",
    "            num_vowels += 1\n",
    "        prev_char = char\n",
    "    \n",
    "    # Handling exceptions like words ending with \"es\" or \"ed\"\n",
    "    if word.endswith('es'):\n",
    "        num_vowels -= 1\n",
    "    elif word.endswith('ed'):\n",
    "        if len(word) >= 4 and word[-3] in vowels:\n",
    "            num_vowels += 1\n",
    "        elif len(word) >= 5 and word[-4] not in vowels:\n",
    "            num_vowels -= 1\n",
    "    \n",
    "    # Avoid negative syllable count\n",
    "    return max(num_vowels, 1)\n",
    "\n",
    "# Function to count syllables in each word of the text\n",
    "def count_syllables_in_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Count syllables in each word\n",
    "    syllable_counts = [count_syllables(word) for word in words]\n",
    "    \n",
    "    return syllable_counts\n",
    "\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "e = df['Text'].apply(count_syllables_in_text)\n",
    "\n",
    "print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51f0ed-25b3-4bff-a05d-670647d476ff",
   "metadata": {},
   "source": [
    "### 12. PERSONAL PRONOUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0031ece4-3b5e-406f-9dc1-1ef4813bf339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2\n",
       "1     4\n",
       "2     2\n",
       "3     1\n",
       "4     1\n",
       "     ..\n",
       "95    4\n",
       "96    3\n",
       "97    1\n",
       "98    1\n",
       "99    1\n",
       "Name: Text, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to count personal pronouns in the text\n",
    "def count_personal_pronouns(text):\n",
    "    # Define the regex pattern to match personal pronouns\n",
    "    pattern = r'\\b(?:I|we|my|ours|us)\\b'\n",
    "    \n",
    "    # Compile the regex pattern\n",
    "    regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Find all matches of personal pronouns in the text\n",
    "    matches = regex.findall(text)\n",
    "    \n",
    "    # Count the number of matches\n",
    "    count = len(matches)\n",
    "    \n",
    "    return count\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "f = df['Text'].apply(count_personal_pronouns)\n",
    "\n",
    "print(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec90b88e-80be-4248-9b62-6a98185ac79a",
   "metadata": {},
   "source": [
    "### 13. AVG WORD LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3eef70-0b27-461c-8e57-dc16187e3185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     5.784461\n",
      "1     6.072712\n",
      "2     6.715292\n",
      "3     6.485060\n",
      "4     6.294910\n",
      "        ...   \n",
      "95    6.311618\n",
      "96    5.648284\n",
      "97    5.969900\n",
      "98    5.585586\n",
      "99    6.044888\n",
      "Name: Text, Length: 100, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate average word length\n",
    "def average_word_length(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Calculate total number of characters in each word\n",
    "    total_char_count = sum(len(word) for word in words)\n",
    "    \n",
    "    # Calculate total number of words\n",
    "    total_word_count = len(words)\n",
    "    \n",
    "    # Calculate average word length\n",
    "    if total_word_count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return total_char_count / total_word_count\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "g = df['Text'].apply(average_word_length)\n",
    "\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca877b-d6e3-497d-a861-66098b711110",
   "metadata": {},
   "source": [
    "# *********************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b994b30-7554-4e92-a928-0fa9d05fac98",
   "metadata": {},
   "source": [
    "## Name - Aatish Kumar Baitha\n",
    "  - M.Tech(Data Science 2nd Year Student)\n",
    "- My Linkedin Profile -\n",
    "  - https://www.linkedin.com/in/aatish-kumar-baitha-ba9523191\n",
    "- My Blog\n",
    "  - https://computersciencedatascience.blogspot.com/\n",
    "- My Github Profile\n",
    "  - https://github.com/Aatishkb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30bdf8a-b699-455a-8a37-07b18581778f",
   "metadata": {},
   "source": [
    "# Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d935503-bfad-408a-ada8-62febffceaab",
   "metadata": {},
   "source": [
    "# *********************************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
